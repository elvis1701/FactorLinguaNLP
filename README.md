
# Translingua
**NMT estilo Transformer com aten√ß√£o hier√°rquica (local + global), embeddings fatorizados e codifica√ß√£o posicional din√¢mica**

> Este reposit√≥rio cont√©m uma implementa√ß√£o educacional e extens√≠vel de um sistema de tradu√ß√£o neural (seq2seq) no estilo Transformer, com **aten√ß√£o hier√°rquica** (janela local + escopo global), **embeddings fatorizados** para reduzir par√¢metros e **positional encoding** **din√¢mico**. Inclui _dataset_ com BOS/EOS, _collate_ com padding, _label smoothing_, _greedy decoding_, e um loop de treinamento completo em PyTorch.

---

## ‚ú® Destaques do projeto
- **Aten√ß√£o hier√°rquica**: combina **aten√ß√£o local** (janela deslizante) com **aten√ß√£o global**. Um _gate_ trein√°vel pondera as duas sa√≠das.
- **Decoder com cross-attention**: utiliza a mem√≥ria do encoder corretamente.
- **M√°scaras corretas e com _batch_first_**: `key_padding_mask` em `(B, L)` e `attn_mask` booleana em `(L, L)`.
- **Positional Encoding din√¢mico**: seno/cosseno com **escala trein√°vel**, forma `(1, L, E)` somada ao embedding.
- **Embeddings fatorizados**: `vocab -> factor_dim -> d_model`, √∫til para vocabul√°rios grandes.
- **Dataset robusto**: adiciona BOS/EOS, trunca para **max_len-2**, e faz padding via `pad_sequence`.
- **Treinamento com Label Smoothing**: perda mais est√°vel e menos _overconfidence_.
- **Decodifica√ß√£o greedy** para infer√™ncia r√°pida.

---

## üìÅ Estrutura do arquivo principal
O arquivo principal √© `translingua.py`, organizado em blocos:

1. **Utilidades de m√°scara**: `causal_mask`, `local_window_mask` e `combine_masks`.
2. **Componentes do modelo**:
   - `DynamicPositionalEncoding`
   - `TokenEmbedding` (fatorizado)
   - `HierarchicalSelfAttention` (local + global com _gate_)
   - `FeedForward`
   - `EncoderBlock` e `DecoderBlock`
   - `TranslinguaModel` (Encoder-Decoder completo)
3. **Dataset/DataLoader**:
   - `TranslationDataset` (com SentencePiece, BOS/EOS e truncamento)
   - `collate_fn` (padding)
4. **Perda**: `LabelSmoothingLoss`
5. **Treinamento**: `train_model(...)`
6. **Infer√™ncia**: `greedy_decode(...)`

---

## üß† Como o algoritmo funciona (vis√£o geral)
### Pipeline de dados
1. **Tokeniza√ß√£o** com SentencePiece (`.model` do idioma fonte e alvo).
2. **Prepara√ß√£o**: para cada senten√ßa, codifica, trunca, adiciona `BOS` e `EOS`.
3. **_Batching_**: padding por `PAD=0` via `collate_fn`.

### Encoder
- **Entrada**: `src` `(B, Ls)` ‚Üí `TokenEmbedding` ‚Üí soma com `PositionalEncoding din√¢mico` ‚Üí `Dropout`.
- **Blocos**: cada `EncoderBlock` aplica
  - `HierarchicalSelfAttention` (sem m√°scara causal; usa apenas `key_padding_mask`).
  - `Residual + LayerNorm`
  - `FeedForward` + `Residual + LayerNorm`
- **Sa√≠da**: mem√≥ria `(B, Ls, d_model)`.

### Decoder
- **Entrada**: `tgt_in` `(B, Lt)` ‚Üí `TokenEmbedding` + `PositionalEncoding din√¢mico`.
- **Self-Attention**: usa **m√°scara causal** (triangular superior) **+** m√°scara **local** de janela (¬±`w`).
- **Cross-Attention**: consulta a **mem√≥ria do encoder** (com `key_padding_mask` da fonte).
- **FFN** + normaliza√ß√µes.
- **Proje√ß√£o final**: `Linear(d_model ‚Üí vocab_tgt)` produz `logits` por passo.

### Aten√ß√£o hier√°rquica (local + global)
- **Local**: `attn_mask` de janela deslizante (¬±`w`) **restringe** o escopo por token.
- **Global**: aten√ß√£o padr√£o sem janela (mas respeitando causal/padding).
- **Combina√ß√£o**: `combined = gate * local + (1 - gate) * global`, com `gate` **trein√°vel** (inicial 0.5).

### M√°scaras
- `key_padding_mask`: `True` marca tokens **PAD** e os mascara.
- `attn_mask`: booleana `(L, L)`; `True` = posi√ß√£o proibida. Usada para **causal** e/ou **janela local**.
- Tudo opera com `batch_first=True`.

---

## üî© Diagrama do fluxo (Encoder ‚Üí Decoder ‚Üí Sa√≠da)

```mermaid
flowchart LR
    A[Texto Fonte] --> B[SentencePiece\nencode + BOS/EOS]
    B --> C[src: Tensor (B,Ls)]
    C --> D[TokenEmbedding (fatorizado)]
    D --> E[PosEnc Din√¢mico (1,L,E) + Dropout]
    E --> F[Encoder x N camadas]
    subgraph ENCODER
      F1[Self-Attn Hier√°rquica\n Local (janela w)\n + Global (full)\n+ Gate trein√°vel]
      F2[Residual + LayerNorm]
      F3[FeedForward + Residual + LayerNorm]
      F --repete N--> F
    end
    F --> G[Mem√≥ria do Encoder (B,Ls,E)]

    H[Texto Alvo (shifted)] --> I[SentencePiece\nencode + BOS/EOS]
    I --> J[tgt_in: Tensor (B,Lt)]
    J --> K[TokenEmbedding (fatorizado)]
    K --> L[PosEnc Din√¢mico + Dropout]

    subgraph DECODER
      L --> M[Self-Attn Hier√°rquica\n (Causal + Janela w)]
      M --> N[Residual + LayerNorm]
      N --> O[Cross-Attn\n Q=Decoder, K/V=Mem. Encoder]
      O --> P[Residual + LayerNorm]
      P --> Q[FeedForward + Residual + LayerNorm]
    end

    Q --> R[Linear(d_model ‚Üí Vocab_Tgt)]
    R --> S[Logits ‚Üí Greedy Decode]
    S --> T[Texto Traduzido]
```

**Modifica√ß√µes destacadas**: blocos com _Self-Attn Hier√°rquica_, _TokenEmbedding (fatorizado)_ e _PosEnc Din√¢mico_.

---

## üõ†Ô∏è Instala√ß√£o e requisitos

### Requisitos m√≠nimos
- Python 3.9+
- GPU CUDA (opcional, mas recomendado)
- Pip

### Instala√ß√£o
```bash
git clone <seu-repo-url>
cd <seu-repo>
python -m venv .venv
# Windows
.venv\Scripts\activate
# Linux/Mac
# source .venv/bin/activate

pip install -r requirements.txt
```

### `requirements.txt` sugerido
> Ajuste vers√µes conforme seu ambiente/CUDA.
```
torch>=2.2.0
sentencepiece>=0.1.99
numpy>=1.24.0
tqdm>=4.66.0
sacrebleu>=2.4.0     # avalia√ß√£o (BLEU/chrF)
matplotlib>=3.8.0    # gr√°ficos de treinamento
```

---

## üß™ Preparando os dados e os modelos SentencePiece
Treine dois modelos SentencePiece (fonte/alvo) **ou** reutilize modelos existentes.

```bash
# Exemplo: treinar vocabul√°rio de 16k para a l√≠ngua fonte
spm_train --input=train.src --model_prefix=src --vocab_size=16000 --model_type=bpe --character_coverage=0.9995

# Para a l√≠ngua alvo
spm_train --input=train.tgt --model_prefix=tgt --vocab_size=16000 --model_type=bpe --character_coverage=0.9995
```

Arquivos esperados no diret√≥rio do projeto:
```
src.model   tgt.model
train.src   train.tgt
```

---

## üöÄ Treinamento

Com o ambiente pronto e os arquivos `*.model` e `train.*` existentes, execute:

```bash
python translingua.py
```

Ou personalize os hiperpar√¢metros chamando a fun√ß√£o `train_model(...)` em um _script_ separado ou REPL:

```python
from translingua import train_model

model = train_model(
    src_model_path="src.model",
    tgt_model_path="tgt.model",
    src_file="train.src",
    tgt_file="train.tgt",
    d_model=512,
    n_heads=8,
    num_layers=6,
    ff_dim=2048,
    dropout=0.1,
    local_window=8,
    batch_size=64,
    max_len=256,
    epochs=10,
    lr=1e-4,
    seed=42,
    device=None,      # "cuda" ou "cpu"
)
```

### Hiperpar√¢metros principais
| Par√¢metro       | Padr√£o | Descri√ß√£o |
|-----------------|--------|-----------|
| `d_model`       | 512    | Dimens√£o dos embeddings e das proje√ß√µes do Transformer |
| `n_heads`       | 8      | N√∫mero de _heads_ de aten√ß√£o |
| `num_layers`    | 6      | N¬∫ de camadas no encoder e no decoder |
| `ff_dim`        | 2048   | Dimens√£o interna da FFN |
| `dropout`       | 0.1    | Dropout em aten√ß√£o/FFN/embeddings |
| `local_window`  | 8      | Raio da janela local (¬±w) na aten√ß√£o hier√°rquica |
| `batch_size`    | 32     | Tamanho do batch |
| `max_len`       | 256    | Tamanho m√°ximo por senten√ßa (com BOS/EOS) |
| `epochs`        | 10     | √âpocas de treinamento |
| `lr`            | 1e-4   | Taxa de aprendizado (AdamW) |

> **Dicas**: aumente `batch_size` e `num_layers` com GPU; ajuste `local_window` se seus dados tiverem depend√™ncias mais longas/curtas.

---

## üîé Infer√™ncia (decodifica√ß√£o _greedy_)

```python
import torch, sentencepiece as spm
from translingua import TranslinguaModel, greedy_decode, PAD

# Carregue modelos SentencePiece
src_spm = spm.SentencePieceProcessor(); src_spm.load("src.model")
tgt_spm = spm.SentencePieceProcessor(); tgt_spm.load("tgt.model")

# Carregue pesos do modelo (se salvou ap√≥s o treino)
# model = torch.load("translingua.pt", map_location="cpu")

# Ou treine e reutilize o objeto retornado
# model = train_model(...)

texto = "This is a small test."
traducao = greedy_decode(model, src_spm, tgt_spm, texto, max_len=64, device=None)
print(traducao)
```

> Para produ√ß√£o, considere **beam search** e penaliza√ß√µes de comprimento/cobertura.

---

## üìà Gr√°ficos e m√©tricas (precis√£o/qualidade)
Para tradu√ß√£o, m√©tricas de precis√£o token-a-token s√£o menos informativas do que m√©tricas de **qualidade de sequ√™ncia**. Recomenda-se:

- **BLEU** e **chrF** via `sacrebleu`.
- (Opcional) **COMET/BLEURT** para avalia√ß√µes baseadas em aprendizado.

### Registrando _loss_ por √©poca
O `train_model` imprime `Loss/token`. Para salvar e **plotar**:

```python
# Exemplo simples: modifique o loop de treino para acumular losses
losses = []
for epoch in range(1, epochs + 1):
    ...
    ppl_loss = total_loss / max(1, total_tokens)
    losses.append(ppl_loss)
    print(f"Epoch {epoch:02d} | Loss/token: {ppl_loss:.4f}")

# Plot
import matplotlib.pyplot as plt
plt.figure()
plt.plot(range(1, len(losses)+1), losses, marker="o")
plt.xlabel("√âpoca"); plt.ylabel("Loss por token"); plt.title("Curva de treinamento")
plt.grid(True); plt.tight_layout()
plt.savefig("training_curve.png")
```

### Avaliando com BLEU/chrF
Supondo que voc√™ possua `dev.src` e `dev.ref` (refer√™ncias) e um _script_ que gera `dev.hyp` com seu modelo:

```bash
# BLEU
sacrebleu dev.ref < dev.hyp

# chrF (mais sens√≠vel a similaridade de _character n-grams_)
sacrebleu -m chrf dev.ref < dev.hyp
```

> **Boas pr√°ticas**: use `sacrebleu` para garantir comparabilidade; n√£o fa√ßa _tokenization_ manual distinta entre sistemas.

---

## üß™ Casos de uso
- **Tradu√ß√£o de dom√≠nio geral**: paralelos gen√©ricos (ex.: not√≠cias, Wikipedia).
- **Adapta√ß√£o de dom√≠nio**: jur√≠dico, m√©dico, e-commerce ‚Äî treine/fine-tune com corpora espec√≠ficos.
- **Legendas e subt√≠tulos**: senten√ßas curtas com janelas locais menores podem acelerar.
- **Sistemas embarcados/edge**: **embeddings fatorizados** reduzem par√¢metros de entrada.
- **Educa√ß√£o/Pesquisa**: estudar efeitos de **aten√ß√£o local** vs **global** com o _gate_.

> **Limita√ß√µes**: sem _beam search_, sem cobertura/penalidades, sem BPE dropout, sem _length normalization_ no _greedy_.

---

## üß© Detalhes de implementa√ß√£o importantes
- `nn.MultiheadAttention(..., batch_first=True)` aceita tensores `(B, L, E)` diretamente.
- **Aten√ß√£o local**: `local_window_mask(L,w)` cria m√°scara booleana `(L,L)` com `True` fora da janela (¬±`w`). Pode ser combinada com causal por `combine_masks`.
- **Causal**: `causal_mask(L)` mascara posi√ß√µes futuras (`triu` com diagonal=1).
- **Cross-attn** no decoder**:** `key_padding_mask=src_keypad` (sem causal).
- **Label smoothing**: reduz overfitting e gradientes extremos, especialmente com vocabul√°rios grandes.
- **Clipping de gradiente**: `clip_grad_norm_(..., max_norm=1.0)` para estabilidade.
- **Seed** reproduz√≠vel**:** `torch.manual_seed(seed)`.

---

## üíæ Salvamento/Carregamento de pesos
```python
# salvar
torch.save(model, "translingua.pt")

# carregar
model = torch.load("translingua.pt", map_location="cpu")
model.eval()
```

---

## üßë‚Äçüíª IDEs recomendadas
- **VS Code**: leve, excelente suporte a Python, Jupyter e debugging.
- **PyCharm** (Community/Professional): refatora√ß√£o avan√ßada, inspe√ß√µes est√°ticas.
- **JupyterLab**: ideal para experimentos r√°pidos e visualiza√ß√µes.
- **VS Code Remote/Dev Containers**: ambiente reprodut√≠vel.

**Extens√µes √∫teis**: Python, Pylance, Jupyter, GitLens, EditorConfig, Markdown All in One.

---

## üîß _Troubleshooting_
- **`RuntimeError: The shape of the 2D attn_mask is not correct`**  
  Garanta que `attn_mask` seja `(L,L)` **booleana** e que `key_padding_mask` seja `(B,L)`.
- **`CUDA out of memory`**  
  Reduza `batch_size`, `d_model` ou `num_layers`; aumente `local_window` para reduzir custo? (local n√£o muda custo global, mas voc√™ pode treinar com menos camadas).
- **Vazamento de tamanho** (senten√ßas muito longas)  
  Ajuste `max_len`; confirme truncamento `max_len-2` para BOS/EOS.
- **Resultados fracos**  
  Aumente `epochs`, use `beam search`, ajuste `lr`, treine SentencePiece com vocabul√°rio maior e cobertura adequada.

---

## üìù Roadmap (sugest√µes)
- [ ] Beam search com _length penalty_
- [ ] _Weight tying_ entre `tgt_embed.projection` e `output_proj`
- [ ] M√°scara local com janelas **dilatadas** ou **din√¢micas**
- [ ] _Relative positional encoding_ (T5/DeBERTa)
- [ ] _Knowledge distillation_ para modelos menores

---

## üìö Refer√™ncias (sugest√µes de leitura)
- Vaswani et al., **"Attention Is All You Need"** (2017).  
- Kudo & Richardson, **"SentencePiece: A simple and language independent subword tokenizer and detokenizer"** (2018).
- Loshchilov & Hutter, **"Decoupled Weight Decay Regularization"** (AdamW, 2019).
- Hendrycks & Gimpel, **"Gaussian Error Linear Units (GELUs)"** (2016/2020).
- Szegedy et al., **Rethinking the Inception Architecture for Computer Vision** (label smoothing, 2016).

---

## üìÑ Licen√ßa
Defina aqui a licen√ßa do seu projeto (ex.: MIT, Apache-2.0).
